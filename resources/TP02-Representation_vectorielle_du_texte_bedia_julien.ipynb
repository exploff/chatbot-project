{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h1>Représentation vectorielle du texte BEDIA JULIEN</h1></center>\n",
    "\n",
    "---\n",
    "## Sujet\n",
    "\n",
    "Dans la leçon précédente, nous avons appris à utiliser plusieurs librairies pour prétraiter du texte. Cependant, nous ne sommes jamais allés au-delà de la *représentation du texte* - nous n'avons manipulé que la représentation elle-même. Si nous voulons effectuer une analyse computationnelle sur le texte, nous devrons concevoir des approches pour convertir le texte en une *représentation numérique*.\n",
    "\n",
    "Dans cette leçon, nous allons explorer deux moyens parmi les plus simples pour générer une représentation numérique à partir d'un texte : le **sac de mots(bag-of-words)** et **tf-idf**.\n",
    "\n",
    "Pour ce faire, nous utiliserons abondamment le paquet `scikit-learn`, car il fournit un cadre agréable pour construire les représentations numériques.\n",
    "\n",
    "## Compétences visées\n",
    "\n",
    "- Découvrir les différentes formes que peuvent revétir un chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4dbc3-52dd-4dc9-8d0d-21ad31808d09",
   "metadata": {},
   "source": [
    "## L'ensemble de données sur les tweets des compagnies aériennes\n",
    "\n",
    "Nous allons travailler avec un ensemble de données constitué de tweets sur les compagnies aériennes américaines. Chaque échantillon est un tweet différent, qui a été posté sur une compagnie aérienne spécifique. Ces tweets peuvent exprimer un \"sentiment\" positif, neutre ou négatif.\n",
    "\n",
    "Notez que ce jeu de données, dans l'ensemble, est bien structuré et a déjà subi un certain nettoyage. Cependant, ce n'est pas la norme dans la science des données de la vie réelle ! Nous avons choisi ce jeu de données afin de pouvoir nous concentrer sur la compréhension des concepts de base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dfe32-60ee-417a-aad9-f68a39533452",
   "metadata": {},
   "source": [
    "Chargez le fichier `airline_tweets.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S -0800')\n",
    "\n",
    "\n",
    "tweets = pd.read_csv('airline_tweets.csv', encoding='utf-8', parse_dates=['tweet_created'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "## Analyse exploratoire des données\n",
    "\n",
    "Avant de procéder à un prétraitement ou à une modélisation, nous devons toujours effectuer une analyse exploratoire des données pour nous faire une idée de l'ensemble de données.\n",
    "\n",
    "Tout d'abord, jetez un coup d'œil aux premières lignes et à toutes les colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tweets :  14640\n",
      "5 premières lignes :               tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
      "Colonnes :  Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de tweets : \", len(tweets))\n",
    "\n",
    "print(\"5 premières lignes : \", tweets.head(5))\n",
    "\n",
    "print(\"Colonnes : \", tweets.head().columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "Nous avons un `tweet_id`, qui identifie de façon unique chaque tweet. Nous avons également un `airline_sentiment`, qui prend les valeurs de `\"positive\"`, `\"negative\"`, ou `\"neutral\"`. Il existe d'autres colonnes indiquant l'auteur du tweet, sa date de création, le fuseau horaire de l'utilisateur, et d'autres encore. La colonne la plus intéressante est la colonne `text` : ce sont les tweets. Jetez un coup d'oeil à quelques-uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 premieres valeurs de la colonnes text (contenu des tweets)  0                  @VirginAmerica What @dhepburn said.\n",
      "1    @VirginAmerica plus you've added commercials t...\n",
      "2    @VirginAmerica I didn't today... Must mean I n...\n",
      "3    @VirginAmerica it's really aggressive to blast...\n",
      "4    @VirginAmerica and it's a really big bad thing...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"5 premieres valeurs de la colonnes text (contenu des tweets) \", tweets['text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "Observez quelles sont les compagnies aériennes qui font l'objet de tweets et combien de tweets ont été affectés à chaque compagnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0400a77f-7e79-44bf-81dc-40bf8953a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Virgin America' 'United' 'Southwest' 'Delta' 'US Airways' 'American']\n",
      "United            3822\n",
      "US Airways        2913\n",
      "American          2759\n",
      "Southwest         2420\n",
      "Delta             2222\n",
      "Virgin America     504\n",
      "Name: airline, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "airline_companies = tweets['airline'].unique()\n",
    "print(airline_companies)\n",
    "\n",
    "# On compte le nombre de tweets par compagnie aérienne\n",
    "tweets_per_airline = tweets['airline'].value_counts()\n",
    "print(tweets_per_airline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c497c-4138-47eb-b959-235b552e7a47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Défi 1 : Apprendre à connaître les données\n",
    "\n",
    "Utilisez `pandas` pour trouver les informations suivantes sur les tweets des compagnies aériennes :\n",
    "\n",
    "* Combien y a-t-il de tweets dans l'ensemble de données ?\n",
    "* Combien de tweets sont positifs, neutres et négatifs ?\n",
    "* Quelle *proportion* de tweets est positive, neutre et négative ?\n",
    "* Faites un diagramme à barres montrant la proportion des sentiments des tweets.\n",
    "* Combien de temps sépare les premiers et les derniers tweets ?\n",
    "* Qu'est-ce qui obtient le plus de retweets : les tweets positifs, négatifs ou neutres ?\n",
    "* Identifiez la compagnie aérienne dont les tweets ont la plus grande proportion de sentiments négatifs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09c12ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tweets :  14640\n",
      "Tweets sentiments : \n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "Tweets sentiments (en pourcentage) : \n",
      "negative    62.691257\n",
      "neutral     21.168033\n",
      "positive    16.140710\n",
      "Name: airline_sentiment, dtype: float64\n",
      "Temps séparants le premier et dernier tweet : \n",
      "7 days 12:17:32\n",
      "Type de tweets qui obtient le plus de retweets : \n",
      "11596    negative\n",
      "Name: airline_sentiment, dtype: object\n",
      "airline_sentiment  negative  neutral  positive  negative_proportion\n",
      "airline                                                            \n",
      "American               1960      463       336             0.710402\n",
      "Delta                   955      723       544             0.429793\n",
      "Southwest              1186      664       570             0.490083\n",
      "US Airways             2263      381       269             0.776862\n",
      "United                 2633      697       492             0.688906\n",
      "Virgin America          181      171       152             0.359127\n",
      "Compagnie aérienne qui obtient la plus grande proportions de tweets négatifs :  US Airways\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Nombre de tweets : \", tweets.count().max())\n",
    "\n",
    "print(\"Tweets sentiments : \")\n",
    "print(tweets['airline_sentiment'].value_counts())\n",
    "\n",
    "print(\"Tweets sentiments (en pourcentage) : \")\n",
    "print(tweets['airline_sentiment'].value_counts(normalize=True) * 100)\n",
    "\n",
    "airline_sentiment = tweets['airline_sentiment'].value_counts(normalize=True)\n",
    "x = list(airline_sentiment.index.values.astype(str))  \n",
    "y = list(airline_sentiment.values * 100)\n",
    "\n",
    "tweets_percentage = pd.DataFrame({'Sentiments': x, 'Pourcentage': y})\n",
    "tweets_percentage.plot.bar(x='Sentiments', y='Pourcentage', rot=0)\n",
    "\n",
    "print(\"Temps séparants le premier et dernier tweet : \")\n",
    "print(tweets['tweet_created'].max() - tweets['tweet_created'].min())\n",
    "\n",
    "print(\"Type de tweets qui obtient le plus de retweets : \")\n",
    "print(tweets[tweets['retweet_count'] == tweets['retweet_count'].max()]['airline_sentiment'])\n",
    "\n",
    "# Création d'un dataframe contenant les sentiments des tweets par compagnie aérienne\n",
    "sentiments_by_airline = tweets.groupby(['airline', 'airline_sentiment'])['tweet_id'].count().unstack()\n",
    "# Calcul de la proportion de tweets négatifs par compagnie aérienne en fonction des autres sentiments\n",
    "sentiments_by_airline['negative_proportion'] = sentiments_by_airline['negative'] / sentiments_by_airline.sum(axis=1)\n",
    "print(sentiments_by_airline)\n",
    "most_negative_airline = sentiments_by_airline['negative_proportion'].idxmax()\n",
    "print(\"Compagnie aérienne qui obtient la plus grande proportions de tweets négatifs : \", most_negative_airline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "## Prétraitement\n",
    "\n",
    "Nous avons passé une grande partie des derniers notebooks à apprendre comment prétraiter les données. Appliquons ce que nous avons appris à ce jeu de données. En regardant certains des tweets ci-dessus, nous pouvons voir que même s'ils sont en assez bon état, nous pouvons leur appliquer un traitement supplémentaire.\n",
    "\n",
    "Dans notre pipeline, nous allons omettre le processus de tokénisation, puisque nous l'effectuerons dans une étape ultérieure. Au lieu de cela, nous allons ajouter quelques nouvelles étapes de prétraitement maintenant que nous travaillons avec des données de médias sociaux. Plus précisément, nous remplacerons tous les hashtags par un jeton \"HASHTAG\" et nous remplacerons tous les utilisateurs (désignés par le symbole \"@\") par un jeton \"USER\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Défi 2 : Création d'un pipeline de prétraitement pour les données des médias sociaux\n",
    "\n",
    "Ecrivez une fonction appelée `preprocess()` qui effectue les opérations suivantes sur une entrée de texte :\n",
    "\n",
    "* Mettre le texte en minuscules.\n",
    "* Remplacer toutes les URL par le jeton \"URL\".\n",
    "* Remplacer tous les nombres par le jeton \"DIGIT\".\n",
    "* Remplacer les hashtags par le jeton \"HASHTAG\".\n",
    "* Remplacer tous les utilisateurs par le jeton \"USER\".\n",
    "* Supprimer les espaces vides.\n",
    "\n",
    "\n",
    "Exécutez votre fonction `preprocess()` sur l'exemple suivant:\n",
    "\n",
    "\"lol @toto and @frankl are like soo 800 #today #left get it on https://fb.com #gogo\"\n",
    "\n",
    "Quand vous pensez qu'elle fonctionne, appliquez-la à la colonne `text` entière dans le DataFrame tweets et le résultat est stockée dans une nouvelle colonne `text_processed`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dfcbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 USER what USER said.\n",
      "1    USER plus you've added commercials to the expe...\n",
      "2    USER i didn't today... must mean i need to tak...\n",
      "3    USER it's really aggressive to blast obnoxious...\n",
      "4        USER and it's a really big bad thing about it\n",
      "Name: text_processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expression régulière pour identifier les URLs\n",
    "    url_regex = r'http\\S+'\n",
    "    text = re.sub(url_regex, \"URL\", text)\n",
    "\n",
    "    # Expression régulière pour identifier les chiffres\n",
    "    digit_regex = r'\\d+'\n",
    "    text = re.sub(digit_regex, \"DIGIT\", text)\n",
    "\n",
    "    # Expression régulière pour identifier les hashtags\n",
    "    hashtag_regex = r'#\\w+'\n",
    "    text = re.sub(hashtag_regex, \"HASHTAG\", text)\n",
    "\n",
    "    # Expression régulière pour identifier les Users\n",
    "    user_regex = r'@\\S+'\n",
    "    text = re.sub(user_regex, \"USER\", text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    # Fonction pour faire la subsititution\n",
    "    return text\n",
    "\n",
    "tweets['text_processed'] = tweets['text'].apply(preprocess)\n",
    "\n",
    "print(tweets['text_processed'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "## La représentation en sac de mots\n",
    "\n",
    "Le principe fondamental du sac de mots est d'encoder le corpus en termes de fréquences de mots. Prenons le cas du sentiment : nous savons que le sentiment est véhiculé plus fortement par des mots spécifiques. Par exemple, si un tweet contient le mot \"heureux\", il est susceptible de véhiculer un sentiment positif (mais pas toujours - quelqu'un pourrait dire qu'il n'est \"pas heureux\" - le sentiment opposé !) En outre, lorsque ces mots reviennent plus souvent, ils véhiculent probablement un sentiment plus fort.\n",
    "\n",
    "Dans un sac de mots, nous prenons un texte, nous le tokenisons, puis nous tabulons les fréquences de chaque token. La représentation numérique du texte est donc un vecteur indiquant les fréquences de chaque token pour ce texte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "### Matrice document-terme\n",
    "\n",
    "Dans la plupart des corpus de textes, nous aurons de nombreux échantillons ou *documents*. Par exemple, dans l'ensemble de données de tweets de compagnies aériennes, nous avons de nombreux tweets. Chaque tweet est un échantillon unique en soi : on peut le considérer comme un document unique dans l'ensemble du *corpus*. Comme ils sont tous liés les uns aux autres, de nombreux tokens peuvent être partagés entre les tweets. Ainsi, lors de la création du modèle de sac de mots, nous pouvons effectuer une tokenisation de tous les documents, formant ainsi un *vocabulaire*. Ensuite, nous pouvons représenter un document unique en fonction des tokens du vocabulaire qui sont représentés, et de leur fréquence dans le document.\n",
    "\n",
    "Si le vocabulaire comporte $V$ tokens, alors chaque document sera encodé dans un vecteur à $V$ dimensions. S'il y a $D$ documents, l'ensemble des données peut être représenté dans une matrice $D \\times V$, où chaque ligne correspond au document, et chaque colonne correspond au token (ou \"terme\"). Cette matrice $D \\times V$ est une **matrice document-terme** (DTM).\n",
    "\n",
    "\n",
    "Pour créer une DTM, nous allons utiliser le `CountVectorizer` du package `sklearn`, une librairie d'apprentissage automatique très utilisé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f41838-a387-43d0-b4f7-c2e0dba4d050",
   "metadata": {},
   "source": [
    "Si vous n'êtes pas familier avec `sklearn`, voici le flux de travail général :\n",
    "\n",
    "1. Nous créons d'abord un objet `CountVectorizer`, et choisissons des paramètres spécifiques pour savoir comment nous allons créer la DTM. Consultez la [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) pour voir quelles options sont disponibles.\n",
    "2. Ensuite, nous \"ajustons\" cet objet `CountVectorizer` aux données. Dans ce contexte, \"ajuster\" consiste à établir un vocabulaire de tokens à partir des documents de votre jeu de données.\n",
    "3. Enfin, nous \"transformons\" les données en fonction de l'objet `CountVectorizer` \"ajusté\". Cela signifie que nous prenons nos données textuelles et les transformons en une DTM selon le vocabulaire établi par l'étape d'ajustement.\n",
    "4. Vous pouvez faire les étapes 2 et 3 d'un seul coup en utilisant une fonction `fit_transform`.\n",
    "\n",
    "Appliquez ce flux à la colonne `text_processed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "dtm = count_vectorizer.fit_transform(tweets['text_processed'])\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036ca73-2d0b-4781-9f2c-a215c838f4b2",
   "metadata": {},
   "source": [
    "Le résultat obtenu n'est pas exatement un tableau `numpy`. A la place, c'est un tableau `numpy` stocké en CSF (Compressed Sparse Format), autrement dit une matrice éparse. Ce format permet d'économiser beaucoup de mémoire, mais il est difficile à visualiser pour un humain. Pour illustrer les techniques de cette leçon, nous allons d'abord décompresser cette matrice. Cependant, pour les ensembles de données plus importants, il est préférable d'éviter cette opération, car l'utilisation du CSF présente des avantages en termes de performances.\n",
    "\n",
    "Convertissez la matrice éparse en un tableau `numpy` normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# conversion en tableau numpy\n",
    "dtm_array = np.array(dtm.toarray())\n",
    "\n",
    "# affichage du tableau numpy\n",
    "print(dtm_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b33def-90fe-4130-9a26-dbee653b9e1c",
   "metadata": {},
   "source": [
    "Il y a beaucoup de zéros ici ! C'est logique : il y a probablement beaucoup de tokens dans le vocabulaire, et chaque tweet n'a probablement que quelques tokens.\n",
    "\n",
    "Il serait bon de savoir quelle colonne fait référence à quels tokens. Créez un DataFrame `pandas` qui contient cette information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       _exact_  aa  aaaand  aadavantage  aadigit  aadigitso  aadv  aadvantage  \\\n",
      "0            0   0       0            0        0          0     0           0   \n",
      "1            0   0       0            0        0          0     0           0   \n",
      "2            0   0       0            0        0          0     0           0   \n",
      "3            0   0       0            0        0          0     0           0   \n",
      "4            0   0       0            0        0          0     0           0   \n",
      "...        ...  ..     ...          ...      ...        ...   ...         ...   \n",
      "14635        0   0       0            0        0          0     0           0   \n",
      "14636        0   0       0            0        0          0     0           0   \n",
      "14637        0   0       0            0        0          0     0           0   \n",
      "14638        0   0       0            0        0          0     0           0   \n",
      "14639        0   0       0            0        0          0     0           0   \n",
      "\n",
      "       aal  aaldigit  ...  zdigit  zero  zig  zip  zippers  zone  zones  zoom  \\\n",
      "0        0         0  ...       0     0    0    0        0     0      0     0   \n",
      "1        0         0  ...       0     0    0    0        0     0      0     0   \n",
      "2        0         0  ...       0     0    0    0        0     0      0     0   \n",
      "3        0         0  ...       0     0    0    0        0     0      0     0   \n",
      "4        0         0  ...       0     0    0    0        0     0      0     0   \n",
      "...    ...       ...  ...     ...   ...  ...  ...      ...   ...    ...   ...   \n",
      "14635    0         0  ...       0     0    0    0        0     0      0     0   \n",
      "14636    0         0  ...       0     0    0    0        0     0      0     0   \n",
      "14637    0         0  ...       0     0    0    0        0     0      0     0   \n",
      "14638    0         0  ...       0     0    0    0        0     0      0     0   \n",
      "14639    0         0  ...       0     0    0    0        0     0      0     0   \n",
      "\n",
      "       zukes  zurich  \n",
      "0          0       0  \n",
      "1          0       0  \n",
      "2          0       0  \n",
      "3          0       0  \n",
      "4          0       0  \n",
      "...      ...     ...  \n",
      "14635      0       0  \n",
      "14636      0       0  \n",
      "14637      0       0  \n",
      "14638      0       0  \n",
      "14639      0       0  \n",
      "\n",
      "[14640 rows x 9761 columns]\n"
     ]
    }
   ],
   "source": [
    "# création du DataFrame des tokens\n",
    "tokens_df = pd.DataFrame(dtm_array)\n",
    "tokens_df.columns = count_vectorizer.get_feature_names_out()\n",
    "# affichage du DataFrame des tokens\n",
    "print(tokens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb9e39-93c0-4301-bf9d-c4243a12d96b",
   "metadata": {},
   "source": [
    "Que pouvons-nous faire avec la DTM ? Tout d'abord, nous pouvons voir les tokens les plus fréquents. \n",
    "\n",
    "Afficher les 10 tokens les plus fréquents avec le nombre d'apparition associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user         16485\n",
      "digit         6220\n",
      "flight        3924\n",
      "hashtag       3470\n",
      "url           1211\n",
      "thanks        1078\n",
      "cancelled     1057\n",
      "just           974\n",
      "service        965\n",
      "help           852\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# les 10 tokens les plus fréquents sans les mots vides\n",
    "print(tokens_df.sum().sort_values(ascending=False)[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64385987",
   "metadata": {},
   "source": [
    "BONUS\n",
    "\n",
    "- Quel token sans hashtag et sans chiffre apparaît le plus souvent dans un tweet donné ? \n",
    "- Combien de fois apparaît-il ? \n",
    "- Quel est le tweet original ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6f80d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    what\n",
      "1    plus\n",
      "2       i\n",
      "3    it's\n",
      "4     and\n",
      "Name: most_common_token, dtype: object\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    1\n",
      "4    1\n",
      "Name: most_common_token_count, dtype: int64\n",
      "Token le plus fréquent : to\n",
      "Tweets correspondants : USER lost bag on flight to vegas. now to el paso. going to dallas love. gate attendant said see if can coordinate to get it to love\n",
      "5180     5\n",
      "220      5\n",
      "3412     5\n",
      "7566     5\n",
      "13715    5\n",
      "537      5\n",
      "10260    5\n",
      "12548    5\n",
      "13520    5\n",
      "13925    5\n",
      "Name: most_common_token_count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julien\\AppData\\Local\\Temp\\ipykernel_3976\\1532080745.py:42: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(t[:10])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_most_common_token(tweet):\n",
    "    # éliminer les tokens qui contiennent des hashtags ou des chiffres\n",
    "    filtered_tokens = [token for token in tweet.split() if not re.search('HASHTAG|DIGIT|USER', token)]\n",
    "    # compter les occurrences des tokens\n",
    "    token_counts = {}\n",
    "    for token in filtered_tokens:\n",
    "        if token in token_counts:\n",
    "            token_counts[token] += 1\n",
    "        else:\n",
    "            token_counts[token] = 1\n",
    "\n",
    "    # trouver le token le plus fréquent\n",
    "    if len(token_counts) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    most_common_token = max(token_counts, key=token_counts.get)\n",
    "    count = token_counts[most_common_token]\n",
    "\n",
    "    return most_common_token, count\n",
    "\n",
    "# Pour chaque tweets on regarde quel token apparait le plus souvent et combien de fois mis à part HASHTAG et DIGIT\n",
    "pair = tweets['text_processed'].apply(get_most_common_token)\n",
    "tweets['most_common_token'] = pair.apply(lambda x: x[0])\n",
    "tweets['most_common_token_count'] = pair.apply(lambda x: x[1])\n",
    "\n",
    "print(tweets['most_common_token'][:5])\n",
    "print(tweets['most_common_token_count'][:5])\n",
    "\n",
    "# Calcul du nombre de fois ou le token le plus fréquent apparait\n",
    "t = tweets['most_common_token_count'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# récupére l'index du token le plus fréquent\n",
    "index = t.index[0]\n",
    "print(\"Token le plus fréquent : \" + tweets['most_common_token'][index])\n",
    "\n",
    "# affiche le tweet correspondant\n",
    "print(\"Tweets correspondants : \" + tweets['text_processed'][index])\n",
    "\n",
    "print(t[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31e1175833a694ff97509d6def20d8edc8a42b356a324552ebe400f6d1092ed7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
