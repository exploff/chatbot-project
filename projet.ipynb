{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet ChatBot\n",
    "\n",
    "## Sommaire\n",
    "\n",
    "- Rappel\n",
    "- Problématique\n",
    "- Sujet\n",
    "- Outils\n",
    "- Modalités\n",
    "- Ressources\n",
    "\n",
    "## Rappel\n",
    "\n",
    "<p>\n",
    "    Lors des 5 travaux précédents, vous avez-vu les bases de la NLP, <br/>\n",
    "    vous avez exploré les différentes manières de traiter du texte et <br/>\n",
    "    vous avez mis en place votre premier chatbot simpliste. \n",
    "</p>\n",
    "\n",
    "## Problématique\n",
    "\n",
    "<p>Le but de ce projet et de mettre en place un chatbot de type OK Google, Siri, Alexa,..</p>\n",
    "<p>Dans ce projet vous aller créer un certain nombre de fonctions qui seront appliquées par un algorithme de reconnaissance vocale.<br>\n",
    "Le programme prend en entrée un texte enregistré à l'aide d'un microphone et en sortie exécute une fonction adéquate à la demande énoncée</p>\n",
    "\n",
    "<i><u>Exemple :</u><br>\n",
    "Moi: \"Montre moi un chat\"<br>\n",
    "ChatBot: \"Ok je vous montre un chat\" => Ouvre un navigateur avec une photo de chat.\n",
    "</i>\n",
    "\n",
    "## Sujet\n",
    "En vous appuyant sur les TP précédemment vu en cours et les outils fournis votre chatbot devra être capable de :\n",
    "- Prendre une entrée microphone\n",
    "- Adapter la langue du chatbot en fonction de la demande en français ou en anglais\n",
    "- Exécuter des fonctions macros à l'aide du fichier de configuration fourni\n",
    "- Pouvoir exécuter au moins 4 fonctions distinctes\n",
    "- Si la demande ne figure pas dans le fichier de configuration: Utiliser un chatbot extérieur afin de tenter de répondre à la demande\n",
    "\n",
    "Fonctionnalités : \n",
    "- Ouvrir un navigateur pour la recherche en général (musique, etc...)\n",
    "- Donner la météo, ou autre chose (API)\n",
    "- Répondre à l'oral\n",
    "- Répondre à une problématique médicale\n",
    "\n",
    "<b>Un soin tout particulié sera donné à la conception</b>\n",
    "\n",
    "> Soigner l'ergonomie\n",
    "\n",
    "> Soigner la qualité du code\n",
    "\n",
    "## Outils\n",
    "Afin de vous aider à réaliser ce projet, je vous invite à regarder les librairies suivantes:\n",
    "- SpeechRecognition<br>\n",
    "https://pypi.org/project/SpeechRecognition/\n",
    "- transformers<br>\n",
    "https://pypi.org/project/transformers/2.1.0/\n",
    "- gTTS<br>\n",
    "https://pypi.org/project/gTTS/\n",
    "\n",
    "## Modalités\n",
    "- Travail en groupe de 2 ou 3\n",
    "- Soutenance <b>technique</b> de fin de projet de 15 minutes avec rapport de 10 pages minimum\n",
    "- A rendre avant le 03.04.2023 23:59\n",
    "- Rapport à rendre avant le 10.04.2023 23:59\n",
    "- Sous le nom: nGroupe_nom1_nom2_nom3_chatbot.ipynb\n",
    "\n",
    "## Ressources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_browse = [\n",
    "    \"open the browser\", \"open the browser\"\n",
    "]\n",
    "\n",
    "intent_news = [\n",
    "    \"to be informed\", \"be up-to-date\", \"be well-informed\", \"be knowledgeable\", \"be briefed\", \"be apprised\", \"be in the loop\", \"be in the know\", \"be versed\", \"be cognizant\", \"be acquainted with\"\n",
    "]\n",
    "\n",
    "intent_medical =  [\n",
    "    \"relieve\", \"alleviate\", \"lessen\", \"ease\", \"mitigate\", \"reduce\", \"diminish\", \"assuage\", \"pacify\", \"soothe\", \"quell\"\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initChatbot():\n",
    "    # DO SOMETHING\n",
    "    print(\"[+] - ChatBot initialized\")\n",
    "def initSpeechRecognition():\n",
    "    # DO SOMETHING\n",
    "    print(\"[+] - Speech Recognition initialized\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    print(\"######### INITIALIZATION #########\")\n",
    "    ## Speech Recognition\n",
    "    print(\"[o] - Initializing Speech Recognition\")\n",
    "    sr = initSpeechRecognition()\n",
    "    ## ChatBot\n",
    "    print(\"[o] - Initializing ChatBot\")\n",
    "    chatbot = initChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### START #########\n",
      "######## EXIT #########\n",
      "[o] - Thanks for using me\n",
      "######### END #########\n"
     ]
    }
   ],
   "source": [
    "working = False\n",
    "print(\"######### START #########\")\n",
    "\n",
    "while(working):\n",
    "    print(\"> Say something\")\n",
    "\n",
    "    # DO SOMETHING\n",
    "\n",
    "    if(\"\"\"SOMETHING\"\"\"):\n",
    "        print(\"[+] - Speech Recognition success\")\n",
    "        print(\">  You said: #SOMETHING\")\n",
    "        # DO SOMEWHAT\n",
    "        \n",
    "    else:\n",
    "        print(\"[-] - Speech Recognition failed\")\n",
    "        print(\"> I didn't catch that.\")\n",
    "        working = False\n",
    "\n",
    "print(\"######## EXIT #########\")\n",
    "print(\"[o] - Thanks for using me\")\n",
    "print(\"######### END #########\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JULIEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Julien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "d:\\Python\\lib\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# Charger le modèle 'en' pour la langue anglaise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(text, strip=True, lowercase=True, cleaning_digit_url=True, remove_stopwords=True, tokenization=True):\n",
    "\n",
    "    if strip:\n",
    "        # strip\n",
    "        text = text.strip()\n",
    "\n",
    "    if lowercase:\n",
    "        # minuscule\n",
    "        text = text.lower()\n",
    "\n",
    "    if cleaning_digit_url:\n",
    "        # Expression régulière pour identifier les URLs\n",
    "        url_regex = r'http\\S+'\n",
    "        text = re.sub(url_regex, \"URL\", text)\n",
    "\n",
    "        # Expression régulière pour identifier les chiffres\n",
    "        digit_regex = r'\\d+'\n",
    "        text = re.sub(digit_regex, \"DIGIT\", text)\n",
    "\n",
    "        # Expression régulière pour identifier les users\n",
    "        user_regex = r'@\\S+'\n",
    "        text = re.sub(user_regex, \"USER\", text)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        # suppression des mots vides\n",
    "        filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "        text = ' '.join(filtered_words)\n",
    "\n",
    "    if tokenization:\n",
    "        # racinisation et lemmatisation\n",
    "        nlp_output = nlp(text)\n",
    "    \n",
    "    return nlp_output\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Supprime toutes les balises HTML d'une chaîne de caractères\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Fonction pour récupérer les dernières actualités à partir de le Monde\n",
    "def get_news():\n",
    "    url = 'https://www.lemonde.fr/'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    news_items = soup.find_all(class_='article__title')\n",
    "    # Delete all balise html in news_items\n",
    "    news = []\n",
    "    for item in news_items:\n",
    "        news.append(remove_html_tags(str(item)))\n",
    "\n",
    "    return '\\n'.join(news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "d:\\Python\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-e2m-intent\")\n",
    "\n",
    "def get_intent(event, max_length=16):\n",
    "  input_text = \"%s </s>\" % event\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "  return remove_html_tags(tokenizer.decode(output[0])).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_similarity_cosinus(question, corpus):\n",
    "    corpus_token = [preprocess(q).text for q in corpus]\n",
    "\n",
    "    phrase = preprocess(question).text\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Calcul du vecteur tf-idf du corpus\n",
    "    tfidf_corpus = vectorizer.fit_transform(corpus_token)\n",
    "\n",
    "    # Calcul du vecteur tf-idf de la phrase d'exemple\n",
    "    tfidf_phrase = vectorizer.transform([phrase])\n",
    "\n",
    "    # Calcul de la similarité cosinus entre la phrase d'exemple et le corpus\n",
    "    cosine_similarities = cosine_similarity(tfidf_phrase, tfidf_corpus).flatten()\n",
    "\n",
    "    # Récupération de l'indice de la phrase la plus similaire\n",
    "    most_similar_index = cosine_similarities.argmax()\n",
    "\n",
    "    return cosine_similarities[most_similar_index]\n",
    "\n",
    "\n",
    "def doAction():\n",
    "    print(\"#####\")\n",
    "\n",
    "event = \"What's the latest news on the world of gaming?\"\n",
    "intent = get_intent(event)\n",
    "\n",
    "print(get_similarity_cosinus(intent, intent_news))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
